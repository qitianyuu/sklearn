{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455ce45d",
   "metadata": {},
   "source": [
    "# 决策树经典算法及部分数学"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0d93a",
   "metadata": {},
   "source": [
    "### 信息熵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a25ffd",
   "metadata": {},
   "source": [
    " $$ H(x) = -\\sum_{i=1}^{n} {p_i\\log{p_i}} \\qquad Entropy(t) = -\\sum_{i=0}^{c-1} {P(i|t)\\log_2p(i|t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb13750",
   "metadata": {},
   "source": [
    "其实上面两个式子相同，都是每个事件发生的概率乘对应的信息熵大小的累加，**熵就是系统信息量的期望** $ \\quad H(P):=E(P_f)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a58534",
   "metadata": {},
   "source": [
    "### 相对熵(KL散度)、交叉熵\n",
    "相对熵是一种量化两种概率分布P和Q之间差异的方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e852cd1",
   "metadata": {},
   "source": [
    "$$\\begin{align} D_{KL}(P||Q) &:= \\sum_{i=1}^{m} {p_i\\left(f_Q(q_i)-f_P(q_i)\\right)}\\\\&=\\sum_{i=1}^{m} {p_i\\left(-\\log_2q_i - (-\\log_2p_i)\\right)}\\\\&=\\sum_{i=1}^{m} {p_i(-\\log_2q_i)} - \\sum_{i=1}^{m} {p_i(-\\log_2p_i)}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244c783",
   "metadata": {},
   "source": [
    "上式可知，后面的部分 $\\sum_{i=1}^{m} {p_i(-\\log_2p_i)}$ 是系统 $P$ 的熵，而根据吉布斯不等式，KL散度肯定是大于等于零的\n",
    "\n",
    "前面的部分就是交叉熵 $H(P,Q) = \\sum_{i=1}^{m} {p_i(-\\log_2q_i)}$ \n",
    "\n",
    "因为想让两个分布的熵相同，则转化为求 $H(P,Q)$ 的最小值，这个式子本身就可以作为损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e21aa0",
   "metadata": {},
   "source": [
    "### ID3算法\n",
    "ID3 以信息熵为基础，采用信息增益作为选择最优的分裂属性的方法，选择熵作为衡量节点纯度的标准"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98342c61",
   "metadata": {},
   "source": [
    "由上面公式可知，熵一定是大于零的，且**信息熵越大，不确定性越高，纯度越低。** 集合中的所有样本均匀混合时，信息熵最大，纯度最低。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9fc70",
   "metadata": {},
   "source": [
    "信息增益指的就是划分可以带来纯度的提高，信息熵的下降。它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e8191",
   "metadata": {},
   "source": [
    "$$ Gain(D,a) = Entropy(D) - \\sum_{i=1}^{k} {\\frac{\\vert D_i\\vert}{\\vert D\\vert}Entropy(D_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbf723",
   "metadata": {},
   "source": [
    "其中， $\\frac{\\vert D_i\\vert}{\\vert D\\vert}$ 是每个子节点的权重，$a$ 是每个属性，$D$ 是父节点，$D_i$ 是子节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860a562",
   "metadata": {},
   "source": [
    "### 优势与不足"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1bcba6",
   "metadata": {},
   "source": [
    "#### 优点\n",
    "- 假设空间包含所有的决策树，搜索空间完整。\n",
    "- 健壮性好，不受噪声影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28552c52",
   "metadata": {},
   "source": [
    "#### 缺点\n",
    "- 只能处理离散型属性，无法处理连续型变量例如身高体重的等\n",
    "- ID3算法对于缺失值没有进行考虑\n",
    "- 没有考虑过拟合的问题\n",
    "- 以信息增益作为评价标准导致倾向于选择取值较多的属性"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
